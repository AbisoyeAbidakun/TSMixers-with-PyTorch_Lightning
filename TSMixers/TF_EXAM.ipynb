{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f288ab-85aa-4485-b001-74664a42f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc747990-7e59-446b-8c51-0bf4ea4d6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = 'gs://time_series_datasets'\n",
    "LOCAL_CACHE_DIR = './data_loader/dataset/'\n",
    "\n",
    "\n",
    "class TSFDataLoader:\n",
    "  \"\"\"Generate data loader from raw data.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, data, batch_size, seq_len, pred_len, feature_type, target='OT'\n",
    "  ):\n",
    "    self.data = data\n",
    "    self.batch_size = batch_size\n",
    "    self.seq_len = seq_len\n",
    "    self.pred_len = pred_len\n",
    "    self.feature_type = feature_type\n",
    "    self.target = target\n",
    "    self.target_slice = slice(0, None)\n",
    "\n",
    "    self._read_data()\n",
    "\n",
    "  def _read_data(self):\n",
    "    \"\"\"Load raw data and split datasets.\"\"\"\n",
    "\n",
    "    # copy data from cloud storage if not exists\n",
    "    if not os.path.isdir(LOCAL_CACHE_DIR):\n",
    "      os.mkdir(LOCAL_CACHE_DIR)\n",
    "\n",
    "    file_name = self.data + '.csv'\n",
    "    cache_filepath = os.path.join(LOCAL_CACHE_DIR, file_name)\n",
    "    if not os.path.isfile(cache_filepath):\n",
    "      tf.io.gfile.copy(\n",
    "          os.path.join(DATA_DIR, file_name), cache_filepath, overwrite=True\n",
    "      )\n",
    "\n",
    "    df_raw = pd.read_csv(cache_filepath)\n",
    "\n",
    "    # S: univariate-univariate, M: multivariate-multivariate, MS:\n",
    "    # multivariate-univariate\n",
    "    df = df_raw.set_index('date')\n",
    "    if self.feature_type == 'S':\n",
    "      df = df[[self.target]]\n",
    "    elif self.feature_type == 'MS':\n",
    "      target_idx = df.columns.get_loc(self.target)\n",
    "      self.target_slice = slice(target_idx, target_idx + 1)\n",
    "\n",
    "    # split train/valid/test\n",
    "    n = len(df)\n",
    "    if self.data.startswith('ETTm'):\n",
    "      train_end = 12 * 30 * 24 * 4\n",
    "      val_end = train_end + 4 * 30 * 24 * 4\n",
    "      test_end = val_end + 4 * 30 * 24 * 4\n",
    "    elif self.data.startswith('ETTh'):\n",
    "      train_end = 12 * 30 * 24\n",
    "      val_end = train_end + 4 * 30 * 24\n",
    "      test_end = val_end + 4 * 30 * 24\n",
    "    else:\n",
    "      train_end = int(n * 0.7)\n",
    "      val_end = n - int(n * 0.2)\n",
    "      test_end = n\n",
    "    train_df = df[:train_end]\n",
    "    val_df = df[train_end - self.seq_len : val_end]\n",
    "    test_df = df[val_end - self.seq_len : test_end]\n",
    "\n",
    "    # standardize by training set\n",
    "    self.scaler = StandardScaler()\n",
    "    self.scaler.fit(train_df.values)\n",
    "\n",
    "    def scale_df(df, scaler):\n",
    "      data = scaler.transform(df.values)\n",
    "      return pd.DataFrame(data, index=df.index, columns=df.columns)\n",
    "\n",
    "    self.train_df = scale_df(train_df, self.scaler)\n",
    "    self.val_df = scale_df(val_df, self.scaler)\n",
    "    self.test_df = scale_df(test_df, self.scaler)\n",
    "    self.n_feature = self.train_df.shape[-1]\n",
    "\n",
    "  def _split_window(self, data):\n",
    "    inputs = data[:, : self.seq_len, :]\n",
    "    labels = data[:, self.seq_len :, self.target_slice]\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "    inputs.set_shape([None, self.seq_len, None])\n",
    "    labels.set_shape([None, self.pred_len, None])\n",
    "    return inputs, labels\n",
    "\n",
    "  def _make_dataset(self, data, shuffle=True):\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=data,\n",
    "        targets=None,\n",
    "        sequence_length=(self.seq_len + self.pred_len),\n",
    "        sequence_stride=1,\n",
    "        shuffle=shuffle,\n",
    "        batch_size=self.batch_size,\n",
    "    )\n",
    "    ds = ds.map(self._split_window)\n",
    "    return ds\n",
    "\n",
    "  def inverse_transform(self, data):\n",
    "    return self.scaler.inverse_transform(data)\n",
    "\n",
    "  def get_train(self, shuffle=True):\n",
    "    return self._make_dataset(self.train_df, shuffle=shuffle)\n",
    "\n",
    "  def get_val(self):\n",
    "    return self._make_dataset(self.val_df, shuffle=False)\n",
    "\n",
    "  def get_test(self):\n",
    "    return self._make_dataset(self.test_df, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77952cb5-ff23-4171-afec-62effb0f99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # load datasets\n",
    "  data_loader = TSFDataLoader(\n",
    "      data='weather',\n",
    "      batch_size=32,\n",
    "      seq_len=336,\n",
    "      pred_len=96,\n",
    "      feature_type='M',\n",
    "      target='OT',\n",
    "  )\n",
    "  train_data = data_loader.get_train()\n",
    "  val_data = data_loader.get_val()\n",
    "  test_data = data_loader.get_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe24c0f6-fdb3-4a89-b8d8-3bd28c7a3a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=(TensorSpec(shape=(None, 336, 21), dtype=tf.float32, name=None), TensorSpec(shape=(None, 96, 21), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40961519-0bca-47a3-88bf-d6cc23a35880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=(TensorSpec(shape=(None, 336, 21), dtype=tf.float32, name=None), TensorSpec(shape=(None, 96, 21), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d28d23dd-8ea4-432b-a8c8-acec3a521f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequences (shape): (4, 2, 21)\n",
      "Output Sequences (shape): (4, 1, 21)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample 3D NumPy array with shape (10, 3, 21)\n",
    "data = np.random.rand(4, 3, 21)\n",
    "\n",
    "# Assuming seq_len = 2 and pred_len = 1 for illustration purposes\n",
    "seq_len = 2\n",
    "pred_len = 1\n",
    "\n",
    "# Creating input and output sequences based on seq_len and pred_len\n",
    "input_sequences = []\n",
    "output_sequences = []\n",
    "\n",
    "# Generating input and output sequences\n",
    "for sequence in data:\n",
    "    for i in range(len(sequence) - seq_len - pred_len + 1):\n",
    "        input_seq = sequence[i:i + seq_len]\n",
    "        output_seq = sequence[i + seq_len:i + seq_len + pred_len]\n",
    "        input_sequences.append(input_seq)\n",
    "        output_sequences.append(output_seq)\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "output_sequences = np.array(output_sequences)\n",
    "\n",
    "print(\"Input Sequences (shape):\", input_sequences.shape)\n",
    "print(\"Output Sequences (shape):\", output_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99c84eeb-c68e-4760-8d75-9c11dc62e475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00316875, 0.4573602 , 0.01919774, 0.67595503, 0.77027054,\n",
       "        0.77914655, 0.08831018, 0.42250614, 0.55844842, 0.59594697,\n",
       "        0.28111921, 0.98381549, 0.96248126, 0.54498142, 0.79147503,\n",
       "        0.617044  , 0.55314048, 0.72282026, 0.96695314, 0.72625082,\n",
       "        0.82753557],\n",
       "       [0.44732178, 0.25866765, 0.24713372, 0.65167408, 0.93386181,\n",
       "        0.51421089, 0.73082274, 0.30311735, 0.85468899, 0.60882467,\n",
       "        0.51449664, 0.10788424, 0.11369338, 0.64138292, 0.81037767,\n",
       "        0.3346115 , 0.89007985, 0.97929528, 0.90432804, 0.08742955,\n",
       "        0.65426042]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bea35e63-d289-4e56-ace7-d6f21821a3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00316875, 0.4573602 , 0.01919774, 0.67595503, 0.77027054,\n",
       "        0.77914655, 0.08831018, 0.42250614, 0.55844842, 0.59594697,\n",
       "        0.28111921, 0.98381549, 0.96248126, 0.54498142, 0.79147503,\n",
       "        0.617044  , 0.55314048, 0.72282026, 0.96695314, 0.72625082,\n",
       "        0.82753557],\n",
       "       [0.44732178, 0.25866765, 0.24713372, 0.65167408, 0.93386181,\n",
       "        0.51421089, 0.73082274, 0.30311735, 0.85468899, 0.60882467,\n",
       "        0.51449664, 0.10788424, 0.11369338, 0.64138292, 0.81037767,\n",
       "        0.3346115 , 0.89007985, 0.97929528, 0.90432804, 0.08742955,\n",
       "        0.65426042],\n",
       "       [0.23035467, 0.90070498, 0.84934115, 0.61073124, 0.88099909,\n",
       "        0.1539288 , 0.35662183, 0.73488949, 0.97908236, 0.46522614,\n",
       "        0.93876556, 0.8944651 , 0.91011926, 0.84441351, 0.89958414,\n",
       "        0.09752477, 0.90620487, 0.59878132, 0.12212723, 0.39242131,\n",
       "        0.10898099]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63ca2d45-619d-417e-8479-e8181ad12328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23035467, 0.90070498, 0.84934115, 0.61073124, 0.88099909,\n",
       "        0.1539288 , 0.35662183, 0.73488949, 0.97908236, 0.46522614,\n",
       "        0.93876556, 0.8944651 , 0.91011926, 0.84441351, 0.89958414,\n",
       "        0.09752477, 0.90620487, 0.59878132, 0.12212723, 0.39242131,\n",
       "        0.10898099]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91a25230-306b-4f66-9f95-3a8fe189ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab33de8-05be-46d6-9716-abbddd5fbcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb11c38-d588-4052-8cfd-1732bea9b7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
